{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8c91668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gtts in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gtts) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\roaming\\python\\python310\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.27->gtts) (2023.11.17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d182a3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp \n",
    "import joblib\n",
    "import numpy as np \n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b184f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "string =''\n",
    "out=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae42a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = {'ع':\"ain\",\n",
    "        'ال':\"al\",\n",
    "        'ا':\"aleff\",\n",
    "        'ب':\"bb\",\n",
    "        'د':\"dal\",\n",
    "        'ظ':\"dha\",\n",
    "        'ض':\"dhad\",\n",
    "        'ف':\"fa\",\n",
    "        'ق':\"gaaf\",\n",
    "        'غ':\"ghain\",\n",
    "        'ه':\"ha\",\n",
    "        'ح':\"haa\",\n",
    "        'ج':\"jeem\",\n",
    "        'ك':\"kaaf\",\n",
    "        'خ':\"khaa\",\n",
    "        'لا':\"la\",\n",
    "        'ل':\"laam\",\n",
    "        'م':\"meem\",\n",
    "        'ن':\"nun\",\n",
    "        'ر':\"ra\",\n",
    "        'ص':\"saad\",\n",
    "        'س':\"seen\",\n",
    "        'ش':\"sheen\",\n",
    "        'ط':\"ta\",\n",
    "        'ت':\"taa\",\n",
    "        'ث':\"thaa\",\n",
    "        'ذ':\"thal\",\n",
    "        'ة':\"toot\",\n",
    "        'و':\"waw\",\n",
    "        'ئ':\"ya\",\n",
    "        'ي':\"yaa\",\n",
    "        'ز':\"zay\"\n",
    "        }\n",
    "def getsymbol(s) : \n",
    "    for i , j in symbol.items() : \n",
    "        if s == j : \n",
    "            return i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70908109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses the MediaPipe library to detect hand landmarks in real-time webcam input\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "\n",
    "# For webcam input:\n",
    "# Specifies that only one hand will be detected\n",
    "# Sets the minimum confidence threshold for hand detection to 0.7\n",
    "hands = mp_hands.Hands(max_num_hands=1,static_image_mode=False, min_detection_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def data_clean(landmark):\n",
    "    data = landmark[0]\n",
    "    data = str(data)\n",
    "    data = data.strip().split('\\n')\n",
    "  \n",
    "    garbage = ['landmark {', '}']\n",
    "  \n",
    "    landmarks = []\n",
    "   \n",
    "    for i in data:\n",
    "        if i not in garbage:\n",
    "            landmarks.append(i)\n",
    "\n",
    "    clean = []\n",
    "\n",
    "    for i in landmarks:\n",
    "        i = i.strip()\n",
    "        clean.append(i[2:])\n",
    "        \n",
    "    return([clean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b5a34a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "while cap.isOpened():\n",
    "  success, image = cap.read()\n",
    "  image = cv2.flip(image, 1)\n",
    "  \n",
    "  if not success:\n",
    "        break\n",
    "\n",
    "  # Flip the image horizontally and convert the BGR image to RGB.\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "  # To improve performance, optionally mark the image as not writeable to pass by reference.\n",
    "  image.flags.writeable = False\n",
    "  # processes the image using the hands object to detect hand landmarks\n",
    "  results = hands.process(image)\n",
    "\n",
    "  # Draw the hand annotations on the image.\n",
    "  image.flags.writeable = True\n",
    "\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# If hand landmarks are detected, draws landmarks and connections on the image\n",
    "  if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "      mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Calls data_clean to clean the detected landmarks\n",
    "    cleaned_landmark = data_clean(results.multi_hand_landmarks)\n",
    "    \n",
    "    # uses the trained model to predict the hand gesture based on the cleaned landmarks\n",
    "    if cleaned_landmark:\n",
    "      clf = joblib.load('ARL_Modelll.pkl')\n",
    "      y_pred = clf.predict(cleaned_landmark)   \n",
    "\n",
    "# responsible for displaying the predicted letter in Arabic on the image. \n",
    "# It converts the predicted label (y_pred[0]) into Arabic text using getsymbol function, \n",
    "# reshapes it for proper display, and then draws it on the image.'''\n",
    "################################################################\n",
    "      out=getsymbol(str(y_pred[0]))    \n",
    "      reshaped_texta = arabic_reshaper.reshape(out)\n",
    "      AR_String = get_display(reshaped_texta)\n",
    "      fontpath = \"arial.ttf\"\n",
    "      fontAR = ImageFont.truetype(fontpath, 62)\n",
    "      img_pil = Image.fromarray(image)\n",
    "      draw = ImageDraw.Draw(img_pil)\n",
    "      draw.text((22,100),AR_String, font = fontAR, fill=(0,0,255) )    \n",
    "      image = np.array(img_pil)\n",
    "##################################################################\n",
    "        \n",
    "\n",
    "# prints the text input by the user in Arabic on the image. \n",
    "# It reshapes the user input string for proper Arabic display, and then draws it on the image.\n",
    "##################################################################\n",
    "  #print the text in arabic\n",
    "  reshaped_text = arabic_reshaper.reshape(string)\n",
    "  bidi_text = get_display(reshaped_text) \n",
    "  fontpath = \"arial.ttf\" \n",
    "  fontA = ImageFont.truetype(fontpath, 36)\n",
    "  img_pil = Image.fromarray(image)\n",
    "  draw = ImageDraw.Draw(img_pil)\n",
    "  draw.text((22,44),bidi_text, font = fontA, fill=(0,255,0) )\n",
    "  image = np.array(img_pil)\n",
    "################################################################\n",
    "  #print the inputs on screen\n",
    "  font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  cv2.putText(image,\"Word:\", (22,34), font, 1, (0,255,0), 2, cv2.LINE_AA)\n",
    "  cv2.putText(image,\"S: Add  D: Delete  Esc: Quit\", (22,470), font, 1, (255,100,100), 2, cv2.LINE_AA)  \n",
    "  cv2.imshow('MediaPipe Hands', image) \n",
    "  \n",
    "  \n",
    "  # Waits for a key press from the user\n",
    "  key = cv2.waitKey(1)  \n",
    "    \n",
    "  # If 's' is pressed, it adds the predicted letter to the string  \n",
    "  if key== ord('s'):     \n",
    "    string = string+\"\"+out  \n",
    "    \n",
    "  if key== ord('d'):\n",
    "    string=string[:-1]  #delete last letter\n",
    "        \n",
    "  if key== 32:  #Space == 32\n",
    "    string = string+\" \";  #add space      \n",
    "        \n",
    "  if key== 27: #Escape == 27\n",
    "    break  # close the application\n",
    "\n",
    "hands.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39e1f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce827cd",
   "metadata": {},
   "source": [
    "we used the gTTS (Google Text-to-Speech) library to convert Arabic text into speech and plays it using the system's default media player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "82daf15d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "No text to speak",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m arabic_text \u001b[38;5;241m=\u001b[39m string\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a gTTS object\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m tts \u001b[38;5;241m=\u001b[39m \u001b[43mgTTS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marabic_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mar\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save the speech as a temporary file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m tts\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gtts\\tts.py:133\u001b[0m, in \u001b[0;36mgTTS.__init__\u001b[1;34m(self, text, tld, lang, slow, lang_check, pre_processor_funcs, tokenizer_func, timeout)\u001b[0m\n\u001b[0;32m    130\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, k, v)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Text\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo text to speak\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;241m=\u001b[39m text\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# Translate URL top-level domain\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: No text to speak"
     ]
    }
   ],
   "source": [
    "# import the gTTS for text-to-speech conversion and os for interacting with the operating system\n",
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# Set the Arabic text\n",
    "arabic_text = string\n",
    "\n",
    "# Create a gTTS object\n",
    "tts = gTTS(text=arabic_text, lang='ar')\n",
    "\n",
    "# Save the speech as a temporary file\n",
    "tts.save(\"output.mp3\")\n",
    "\n",
    "# Play the speech using your system's default media player\n",
    "os.system(\"start output.mp3\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
